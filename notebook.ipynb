{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi  # For Public Videos Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transcript_details(youtube_video_url):\n",
    "    try:\n",
    "        video_id = youtube_video_url.split(\"=\")[1]\n",
    "        transcript_text = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        transcript = \"\"\n",
    "        for i in transcript_text:\n",
    "            transcript += \" \" + i[\"text\"]\n",
    "\n",
    "        return transcript\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "\n",
    "def extract_transcript_details(youtube_video_url):\n",
    "    try:\n",
    "        # Extract the video ID from the URL\n",
    "        video_id = youtube_video_url.split(\"=\")[1]\n",
    "\n",
    "        # List all available transcripts for the video\n",
    "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "\n",
    "        # Try to get the manually created transcript first\n",
    "        transcript = \"\"\n",
    "        languages = ['en', 'hi', 'ta', 'te', 'bn', 'gu', 'fr', 'de', 'es']\n",
    "        try:\n",
    "            transcript_obj = transcript_list.find_manually_created_transcript(\n",
    "                languages)\n",
    "            transcript_text = transcript_obj.fetch()\n",
    "            for item in transcript_text:\n",
    "                transcript += \" \" + item[\"text\"]\n",
    "\n",
    "        # If no manually created transcript is available, fallback to auto-generated\n",
    "        except NoTranscriptFound:\n",
    "            transcript_obj = transcript_list.find_generated_transcript(\n",
    "                languages)\n",
    "            transcript_text = transcript_obj.fetch()\n",
    "            for item in transcript_text:\n",
    "                transcript += \" \" + item[\"text\"]\n",
    "\n",
    "        return transcript\n",
    "\n",
    "    except TranscriptsDisabled:\n",
    "        raise Exception(\"Transcripts are disabled for this video.\")\n",
    "    except NoTranscriptFound:\n",
    "        raise Exception(\"No transcripts were found for this video.\")\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = extract_transcript_details(\"https://www.youtube.com/watch?v=xbnHfRPFvyc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " नमस्कार वेलकम टू स्टडी आ मैं हूं प्रशांत धवन और इस वीडियो में हम बात करने वाले हैं बहुत ही इंपोर्टेंट टॉपिक के बारे में हाल ही में पाकिस्तानी मीडिया ने हर जगह यह रिपोर्ट फैला दी कि पाकिस्तान में बहुत ही बड़े स्केल पर ऑयल और नेचुरल गैस के रिजर्व्स मिल चुके हैं नाउ इन्होंने यहां तक कह दिया यह जो आप लाइन देख रहे हैं यह सब पाकिस्तान मीडिया से उठाई गई है यहां पर लिखा है कि इतना ऑयल मिला है पाकिस्तान में जिससे पाकिस्तान की त तकदीर बदल सकती है इट कैन चेंज द डेस्टिनी ऑफ पाकिस्तान कई पाकिस्तानी न्यूज़ आउटलेट्स यह क्लेम कर रहे हैं कि दुनिया में चौथे सबसे बड़े ऑयल रिजर्व मिले हैं पाकिस्तान को अपने कोस्टल एरिया के पास नाउ मुझे यहां पर यह बोलना पड़ेगा एक देश जब और बाकी सेक्टर्स में फेल कर गया हो कोई आईटी इंडस्ट्री ना हो कोई मैन्युफैक्चरिंग ना हो कोई सर्विस इंडस्ट्री ना हो कुछ बेसिकली आपके कंट्री को इंटरनेशनली किसी चीज के लिए किसी चीज के एक्सपोर्ट के लिए जाना नहीं जाता सिर्फ लिटरली आपका कंट्री फेमस है टेररिज्म के लिए तो काफी बार आप कह लीजिए डिस्कवरी के कहीं से कुछ मिल जाए कहीं से ऑयल मिल जाए कहीं से नेचुरल गैस मिल जाए जिसको बेचकर एक कंट्री अपना कर्ज खत्म कर सकती है ऐसे ख्वाब एक कंट्री देखना स्टार्ट कर देता है तो लिटरली यहां पर यही कहा जा रहा है कि पाकिस्तान को उम्मीद है कि अचानक से इनको इतना तेल मिल गया है कि पूरी तकदीर पाकिस्तान की बदल जाएगी नाउ आप में से कुछ लोग हो सकता है कहे कि डेजा वू की फीलिंग आ रही है डेजा वू होता है कि यह सुना सुना लग रहा है ऐसा लग रहा है कि आज से बहुत टाइम पहले सेम खबर सब कुछ एग्जैक्ट वर्ड्स कहीं पर सुने थे जी हां बिल्कुल अगर आपको डेजा वू की फीलिंग आ रही है तो मैं आपको बता दूं 2018 में भी पाकिस्तान के जब उस वक्त पीएम थे इमरान खान इन्होंने ओपनली ये बोल दिया था कि पाकिस्तान को ऑयल और नेचुरल गैस की बड़ी डिस्कवरी हुई है उस वक्त पाकिस्तान के लोग बहुत ज्यारा खुश हो रहे थे उनको लग रहा था कि यह लकी आदमी बन गया है इनका प्राइम मिनिस्टर और इस आदमी की किस्मत इतनी अच्छी है वर्ल्ड कप जीत लेता है ऑक्सफोर्ड में पड़ा हुआ है और ओबवियसली जब यह पीएम बना तो इसने कुछ ऐसी पॉलिसी बनाई होगी कि अचानक से पाकिस्तान को ऑयल मिल गया नेचुरल गैस मिल गई बट इन दी एंड जब खुदाई हुई जब पता करने की कोशिश करी गई कि क्या कराची के कोस्ट पे कराची के कोस्ट में ऑयल नेचुरल गैस है या नहीं तो कुछ भी यहां पर ना मिला और उसके बाद आपने देखा होगा 2019 में ऐसे आर्टिकल हुए थे पब्लिश बैड न्यूज़ फॉर प्राइम मिनिस्टर इमरान खान नो ऑयल गैस रिजर्व्स फाउंड इन पाकिस्तान ऑफशोर तो मुझे लग रहा है देखिए पाकिस्तान इस बार ना एक टाइप का इंटरनेशनल स्कैम करने की कोशिश कर रहा है हो यहां पर क्या रहा है कि अभी के लिए कोई ऑयल नेचुरल गैस डिस्कवर हुए नहीं है मगर पाकिस्तान का मीडिया बार-बार ऐसी रिपोर्ट फैला रहा है और उसके बाद इनके पीएम ने यह कह दिया कि देखिए हमें 5 बिलियन डॉलर चाहिए होंगे और इनको मिल भी जाएंगे इनको उम्मीद है मिल जाएंगे ये पैसे आने वाले 3 सालों में अब यहां पे आप कहोगे 5 बिलियन डॉलर किस बात के समझाता हूं आपको को पाकिस्तान की तरफ से जब बार-बार इस बात को लेकर रिपोर्ट्स आती हैं कि यहां पे नेचुरल गैस हो सकता है ऑयल यहां पर हो सकती है तो कई इन्वेस्टर्स थोड़े बहुत इंटरेस्टेड हो जाते हैं और पाकिस्तान सिंपली इन इन्वेस्टर्स को यह कहता है कि देखो बिल्कुल ऑयल हो सकता है इतना ऑयल हो सकता है कि वेनेजुएला जैसे कंट्रीज को टक्कर दे रहा होगा पाकिस्तान ऑयल रिजर्व के मामले में बस नीड यह है के द एक्सप्लोरेशन के जब आपको पता है कि देखिए यहां पे ऑयल है मगर एगजैक्टली कहां पे है उसको आप एक्स्ट्रेट कैसे करोगे इसके लिए भी इनिशियली आपको बहुत सारा पैसा चाहिए होता है और यहां पे आप देख पाओगे एक्सप्लोरेशन अलोन रिक्वायर्ड ए हेफ्टी इन्वेस्टमेंट ऑफ अराउंड 5 बिलियन डॉलर्स और साथ ही साथ इसमें चार से पाच साल लग सकते हैं इस ऑयल को एक्सट्रैक्ट करने में अपनी ऑफशोर लोकेशन से और और शहबाज शरीफ ने आज से एक महीने पहले ये बात ओपनली कही थी कि 5 बिलियन डॉलर्स इनके पास प्लेज हो चुके हैं इनके हिसाब से अगेन मुझे पता नहीं है असल में ये हुआ नहीं हुआ क्योंकि काफी बार कुछ बोल देते हैं इन रियलिटी होता कुछ और है पाकिस्तान के पीएम ने कहा था कि वो लकी इन्वेस्टर्स जिनको पाकिस्तान में निवेश करने का मौका मिला है उन्होंने अपनी तरफ से 5 बिलियन डॉलर की इन्वेस्टमेंट कर दी है प्लेज तो होता यहां पे सिंपली क्या है यहां पे ऑब् वियस थोड़ा बहुत करप्शन होगा पाकिस्तान की मिलिट्री इनके जो सरकार के ऑफिशल्स हैं वो थोड़ा बहुत पैसा खाएंगे फिर घूम फिर के इन्वेस्टर का पैसा थोड़ा बहुत लगाया जाएगा ऑयल नेचुरल गैस की एक्सप्लोरेशन में बाकी देखिए कुछ मिले या ना मिले उसके के बारे में मैं कुछ कह नहीं सकता इमरान खान का केस मुझे प्रॉपर्ली याद है इन्होंने भी कुछ इन्वेस्टर्स को फंसाने की कोशिश करी थी इनके टाइम में 5 बिलियन डॉलर वाले इन्वेस्टर फंसे नहीं थे कुछ मिलियंस का ही इन्वेस्टमेंट हुआ था उसमें ही पता लग गया था कि जो भी ये सपने देख रहे थे कि कराची के कोस्ट में ऑयल है नेचुरल गैस है कुछ भी ऐसा रियलिटी में नहीं हुआ तो इस बार देखिए सेम स्क्रिप्ट है पाकिस्तान दुराने की कोशिश कर रहा है देखना यह होगा कि यह 5 बिलियन डॉलर की इन्वेस्टमेंट जिसका यह क्लेम कर रहे हैं कि यह पैसा पाकिस्तान के थ्रू जाएगा और यूज किया जाएगा ऑयल नेचुरल गैस की एक्सप्लोरेशन में क्या यह जेनुइनली होता भी है या फिर नहीं होता दिस विल बी वेरी इंटरेस्टिंग टू सी और अगर यहां पर आपका सवाल यह है कि मान लो हाइपोथेटिकली स्पीकिंग कि पाकिस्तान में जेनुइनली ऑयल मिल गया क्या पाकिस्तान रातों रात सऊदी अरेबिया बन जाएगा अगेन यह बहुत बड़ा फफ है उस केस में भी मैं वैसे आपको यह बता दूं सऊदी अरेबिया की सबसे बड़ी कंपनी है इनकी सरकारी ऑयल नेचुरल गैस की कंपनी अराम को ग्रामको ने ऑलरेडी 40 पर स्टेक पाकिस्तान गैस एंड ऑयल जो इनकी सरकारी कंपनी है गैस एंड ऑयल पाकिस्तान जिसको यह गो कहते हैं इसमें ऑलरेडी परचेस कर रखा है तो मेन जो यहां पर स्टेक होल्डर्स हैं ये ऑलरेडी सऊदी अरेबियन कंपनीज हैं अगर कुछ प्रॉफिट होता भी है तो उसका बड़ा परसेंटेज सऊदी अरेबिया खाएगा कुछ स्टेक बताया जाता है यूएई के इन्वेस्टर्स का भी है तो अगेन देखो ऐसा नहीं है कि पाकिस्तान की डेस्टिनी चेंज होने वाली है और अचानक से सब कुछ चेंज हो जाएगा पाकिस्तान के लिए और जो भी इनके कर्जे हैं यह चुकाना शुरू कर देंगे इस ऑयल की एक्सप्लोरेशन के बाद बट देखना यह होगा क्या क्या फिर से जो इमरान खान के अंडर पाकिस्तान ने किया था सेम चीज क्या यहां पे रिपीट होती है क्या सच में ग्लोबल इन्वेस्टर्स इतने वे मुझे वर्ड यहां पर यूज करना पड़ेगा बुद्धू हैं कि एक चीज जब वोह देख चुके हैं 2019 में फिर से एक हाईप पे बिलीव करके क्या वह 5 बिलियन डॉलर्स पाकिस्तान में प्लेज करेंगे फॉर ऑयल एंड नेचुरल गैस एक्सप्लोरेशन यह बहुत ही इंटरेस्टिंग होगा इस टॉपिक से रिलेटेड कुछ डेवलपमेंट्स आती हैं तो मैं आपके लिए बाद में अपडेटेड वीडियो लाता रहूंगा और फाइनली यहां पे आपसे पूछूंगा एक सवाल देखिए पहला इंटरनेशनल सोलर फेस्टिवल यह ऑर्गेनाइज किया गया था कहां पर रिसेंटली यह बहुत ही ज्यादा न्यूज़ में था ये है यहां पे आपकी ऑप्शंस जयपुर भोपाल न्यू दिल्ली या फिर चेन्नई पीएम मोदी ने भी किया था यहां पर पार्टिसिपेट बताइए पहला इंटरनेशनल सोलर फेस्टिवल ये ऑर्गेनाइज कहां पर किया गया था जो लोग सही आंसर देंगे उनकी कमेंट को मैं हार्ड दे दूंगा ताकि बाकी लोग समझ पाएं कि सही आंसर क्या है और फाइनली यहां पर मैं यह भी ऐड कर दूं कि अगर आप यूपीएससी एग्जाम के लिए प्रिपेयर करना चाहते हैं आपके पास समय है अभी तो स्टडी आईक्यू का लॉन्ग टर्म यूपीएससी कोर्स आप कर सकते हैं परचेज सिंपली कमेंट सेक्शन में जाइए यहां पे आपको मिलेगा ये लिंक इस लिंक पर करिए क्लिक और यहां पे आप परचेज कर पाएंगे स्टडी आईक्यू का लॉन्ग टर्म यूपीएससी कोर्स जो प्राइस यहां पर लिखा है नेवर एवर पे दिस प्राइस सिंपली यहां पर कोड डालिए पीडी 10 और प्राइसेस गिरना शुरू हो जाएंगे एक डिस्काउंटेड रेट पे आप यह कोर्स कर सकते हैं परचेज सो यूज द कोड पीज 10 एंड सेव मनी सो इसी नोट के साथ यह है वीडियो का अं थैंक यू फॉर लिनिंग आई लवेज से मे द गड वच\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0,\n",
    "    groq_api_key=os.getenv(\"groq_api_key\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an Youtube video summarizer. Take the transcript, \n",
    "summarize the video and also return important pointer\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content(transcript_text, prompt):\n",
    "    model = llm\n",
    "    response = model.invoke(prompt+transcript_text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=generate_content(transcript_text=result,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Video Summary:**\\n\\nThe video discusses the attention mechanism in transformers, a key component of large language models. The attention mechanism allows the model to focus on specific parts of the input text when generating the next word. The video explains how the attention mechanism works, including the use of query, key, and value matrices to compute attention patterns. It also discusses how the attention mechanism is used in multi-headed attention, where multiple attention heads are run in parallel to capture different aspects of the input text.\\n\\nThe video uses a simple example to illustrate how the attention mechanism works, where adjectives update the meaning of nouns. It also discusses how the attention mechanism is used in more complex scenarios, such as capturing the relationships between words in a sentence.\\n\\nThe video also touches on the history of the attention mechanism and how it has evolved over time. It mentions that the attention mechanism was first introduced in the 2017 paper \"Attention is All You Need\" and has since become a key component of many large language models.\\n\\n**Important Pointers:**\\n\\n1. **Attention Mechanism:** The attention mechanism is a key component of transformers that allows the model to focus on specific parts of the input text when generating the next word.\\n2. **Query, Key, and Value Matrices:** The attention mechanism uses three matrices: query, key, and value matrices, to compute attention patterns.\\n3. **Multi-Headed Attention:** Multi-headed attention is a technique where multiple attention heads are run in parallel to capture different aspects of the input text.\\n4. **Attention Patterns:** Attention patterns are computed by taking the dot product of the query and key matrices and applying a softmax function to the result.\\n5. **Value Matrix:** The value matrix is used to compute the value vectors that are used to update the embeddings.\\n6. **Masking:** Masking is a technique used to prevent later tokens from influencing earlier tokens in the input text.\\n7. **Parallelizability:** The attention mechanism is highly parallelizable, making it well-suited for large-scale language models.\\n8. **GPT-3:** The video mentions GPT-3, a large language model that uses the attention mechanism, and discusses its architecture and parameter count.\\n\\n**Key Concepts:**\\n\\n1. **Transformers:** Transformers are a type of neural network architecture that uses self-attention mechanisms to process input sequences.\\n2. **Self-Attention:** Self-attention is a mechanism that allows the model to attend to different parts of the input sequence when generating the next word.\\n3. **Embeddings:** Embeddings are high-dimensional vectors that represent the input words or tokens.\\n4. **Contextual Meaning:** Contextual meaning refers to the meaning of a word or token in the context of the surrounding words or tokens.\\n\\n**Key Takeaways:**\\n\\n1. The attention mechanism is a key component of transformers that allows the model to focus on specific parts of the input text when generating the next word.\\n2. The attention mechanism uses query, key, and value matrices to compute attention patterns.\\n3. Multi-headed attention is a technique that allows the model to capture different aspects of the input text.\\n4. The attention mechanism is highly parallelizable, making it well-suited for large-scale language models.', response_metadata={'token_usage': {'completion_tokens': 653, 'prompt_tokens': 5882, 'total_tokens': 6535, 'completion_time': 2.617052025, 'prompt_time': 1.396150939, 'queue_time': 0.006101855000000045, 'total_time': 4.013202964}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run-fca25c41-b4c1-42c2-b7e3-6825fc40335f-0', usage_metadata={'input_tokens': 5882, 'output_tokens': 653, 'total_tokens': 6535})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Video Summary:**\\n\\nThe video discusses the attention mechanism in transformers, a key component of large language models. The attention mechanism allows the model to focus on specific parts of the input text when generating the next word. The video explains how the attention mechanism works, including the use of query, key, and value matrices to compute attention patterns. It also discusses how the attention mechanism is used in multi-headed attention, where multiple attention heads are run in parallel to capture different aspects of the input text.\\n\\nThe video uses a simple example to illustrate how the attention mechanism works, where adjectives update the meaning of nouns. It also discusses how the attention mechanism is used in more complex scenarios, such as capturing the relationships between words in a sentence.\\n\\nThe video also touches on the history of the attention mechanism and how it has evolved over time. It mentions that the attention mechanism was first introduced in the 2017 paper \"Attention is All You Need\" and has since become a key component of many large language models.\\n\\n**Important Pointers:**\\n\\n1. **Attention Mechanism:** The attention mechanism is a key component of transformers that allows the model to focus on specific parts of the input text when generating the next word.\\n2. **Query, Key, and Value Matrices:** The attention mechanism uses three matrices: query, key, and value matrices, to compute attention patterns.\\n3. **Multi-Headed Attention:** Multi-headed attention is a technique where multiple attention heads are run in parallel to capture different aspects of the input text.\\n4. **Attention Patterns:** Attention patterns are computed by taking the dot product of the query and key matrices and applying a softmax function to the result.\\n5. **Value Matrix:** The value matrix is used to compute the value vectors that are used to update the embeddings.\\n6. **Masking:** Masking is a technique used to prevent later tokens from influencing earlier tokens in the input text.\\n7. **Parallelizability:** The attention mechanism is highly parallelizable, making it well-suited for large-scale language models.\\n8. **GPT-3:** The video mentions GPT-3, a large language model that uses the attention mechanism, and discusses its architecture and parameter count.\\n\\n**Key Concepts:**\\n\\n1. **Transformers:** Transformers are a type of neural network architecture that uses self-attention mechanisms to process input sequences.\\n2. **Self-Attention:** Self-attention is a mechanism that allows the model to attend to different parts of the input sequence when generating the next word.\\n3. **Embeddings:** Embeddings are high-dimensional vectors that represent the input words or tokens.\\n4. **Contextual Meaning:** Contextual meaning refers to the meaning of a word or token in the context of the surrounding words or tokens.\\n\\n**Key Takeaways:**\\n\\n1. The attention mechanism is a key component of transformers that allows the model to focus on specific parts of the input text when generating the next word.\\n2. The attention mechanism uses query, key, and value matrices to compute attention patterns.\\n3. Multi-headed attention is a technique that allows the model to capture different aspects of the input text.\\n4. The attention mechanism is highly parallelizable, making it well-suited for large-scale language models.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Video Summary:**\n",
      "\n",
      "The video discusses the attention mechanism in transformers, a key component of large language models. The attention mechanism allows the model to focus on specific parts of the input text when generating the next word. The video explains how the attention mechanism works, including the use of query, key, and value matrices to compute attention patterns. It also discusses how the attention mechanism is used in multi-headed attention, where multiple attention heads are run in parallel to capture different aspects of the input text.\n",
      "\n",
      "The video uses a simple example to illustrate how the attention mechanism works, where adjectives update the meaning of nouns. It also discusses how the attention mechanism is used in more complex scenarios, such as capturing the relationships between words in a sentence.\n",
      "\n",
      "The video also touches on the history of the attention mechanism and how it has evolved over time. It mentions that the attention mechanism was first introduced in the 2017 paper \"Attention is All You Need\" and has since become a key component of many large language models.\n",
      "\n",
      "**Important Pointers:**\n",
      "\n",
      "1. **Attention Mechanism:** The attention mechanism is a key component of transformers that allows the model to focus on specific parts of the input text when generating the next word.\n",
      "2. **Query, Key, and Value Matrices:** The attention mechanism uses three matrices: query, key, and value matrices, to compute attention patterns.\n",
      "3. **Multi-Headed Attention:** Multi-headed attention is a technique where multiple attention heads are run in parallel to capture different aspects of the input text.\n",
      "4. **Attention Patterns:** Attention patterns are computed by taking the dot product of the query and key matrices and applying a softmax function to the result.\n",
      "5. **Value Matrix:** The value matrix is used to compute the value vectors that are used to update the embeddings.\n",
      "6. **Masking:** Masking is a technique used to prevent later tokens from influencing earlier tokens in the input text.\n",
      "7. **Parallelizability:** The attention mechanism is highly parallelizable, making it well-suited for large-scale language models.\n",
      "8. **GPT-3:** The video mentions GPT-3, a large language model that uses the attention mechanism, and discusses its architecture and parameter count.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "1. **Transformers:** Transformers are a type of neural network architecture that uses self-attention mechanisms to process input sequences.\n",
      "2. **Self-Attention:** Self-attention is a mechanism that allows the model to attend to different parts of the input sequence when generating the next word.\n",
      "3. **Embeddings:** Embeddings are high-dimensional vectors that represent the input words or tokens.\n",
      "4. **Contextual Meaning:** Contextual meaning refers to the meaning of a word or token in the context of the surrounding words or tokens.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "1. The attention mechanism is a key component of transformers that allows the model to focus on specific parts of the input text when generating the next word.\n",
      "2. The attention mechanism uses query, key, and value matrices to compute attention patterns.\n",
      "3. Multi-headed attention is a technique that allows the model to capture different aspects of the input text.\n",
      "4. The attention mechanism is highly parallelizable, making it well-suited for large-scale language models.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
